{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V SIMPLE ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data('mnist.npz')\n",
    "train_images = train_images.reshape((60000,28*28)).astype('float32')/255\n",
    "test_images = test_images.reshape((10000,28*28)).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape= (28*28,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 9s 21ms/step - loss: 0.4310 - accuracy: 0.8649 - val_loss: 0.1546 - val_accuracy: 0.9533\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 7s 19ms/step - loss: 0.1969 - accuracy: 0.9420 - val_loss: 0.1126 - val_accuracy: 0.9668\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.1477 - accuracy: 0.9560 - val_loss: 0.0991 - val_accuracy: 0.9703\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1271 - accuracy: 0.9615 - val_loss: 0.0880 - val_accuracy: 0.9732\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1114 - accuracy: 0.9658 - val_loss: 0.0894 - val_accuracy: 0.9727\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.0977 - accuracy: 0.9706 - val_loss: 0.0785 - val_accuracy: 0.9771\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.0882 - accuracy: 0.9721 - val_loss: 0.0753 - val_accuracy: 0.9783\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.0830 - accuracy: 0.9741 - val_loss: 0.0785 - val_accuracy: 0.9774\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.0774 - accuracy: 0.9758 - val_loss: 0.0773 - val_accuracy: 0.9793\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.0698 - accuracy: 0.9781 - val_loss: 0.0754 - val_accuracy: 0.9795\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0664 - accuracy: 0.9796\n",
      "Test accuracy:0.9796000123023987\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "#Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy:{test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 43s 110ms/step - loss: 0.2820 - accuracy: 0.9158 - val_loss: 0.1025 - val_accuracy: 0.9695\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 46s 122ms/step - loss: 0.0741 - accuracy: 0.9774 - val_loss: 0.0611 - val_accuracy: 0.9820\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 50s 132ms/step - loss: 0.0506 - accuracy: 0.9844 - val_loss: 0.0504 - val_accuracy: 0.9857\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 38s 102ms/step - loss: 0.0401 - accuracy: 0.9874 - val_loss: 0.0421 - val_accuracy: 0.9871\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 49s 130ms/step - loss: 0.0321 - accuracy: 0.9904 - val_loss: 0.0447 - val_accuracy: 0.9868\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 44s 117ms/step - loss: 0.0257 - accuracy: 0.9920 - val_loss: 0.0429 - val_accuracy: 0.9872\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 44s 117ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.0438 - val_accuracy: 0.9881\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 35s 95ms/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.0427 - val_accuracy: 0.9887\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 32s 84ms/step - loss: 0.0159 - accuracy: 0.9945 - val_loss: 0.0399 - val_accuracy: 0.9892\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 44s 116ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0353 - val_accuracy: 0.9908\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.0319 - accuracy: 0.9905\n",
      "Test accuracy:Â 0.9904999732971191\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the dataset from online\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the pixel values\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Convert labels to categorical format\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build the neural network\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D(2, 2))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2, 2))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "#Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f'Test accuracy:{test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_data(folder):\n",
    "    all_train_images, all_train_labels = [], []\n",
    "\n",
    "    # Load training data from all batches\n",
    "    for batch_num in range(1, 6):\n",
    "        batch_filename = f'{folder}/data_batch_{batch_num}'\n",
    "        with open(batch_filename, 'rb') as fo:\n",
    "            batch = pickle.load(fo, encoding='bytes')\n",
    "            train_images = batch[b'data']\n",
    "            train_labels = np.array(batch[b'labels'])\n",
    "\n",
    "            all_train_images.append(train_images)\n",
    "            all_train_labels.append(train_labels)\n",
    "\n",
    "    # Load test data\n",
    "    with open(f'{folder}/test_batch', 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "        test_images = batch[b'data']\n",
    "        test_labels = np.array(batch[b'labels'])\n",
    "\n",
    "    # Concatenate data from all batches\n",
    "    train_images = np.concatenate(all_train_images, axis=0)\n",
    "    train_labels = np.concatenate(all_train_labels, axis=0)\n",
    "\n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "(train_images, train_labels), (test_images, test_labels) = load_cifar10_data('cifar-10-batches-py')\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 22s 64ms/step - loss: 2.1686 - accuracy: 0.1855 - val_loss: 2.0076 - val_accuracy: 0.2682\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 2.0496 - accuracy: 0.2216 - val_loss: 1.9771 - val_accuracy: 0.2926\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 2.0129 - accuracy: 0.2316 - val_loss: 1.9379 - val_accuracy: 0.3042\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 13s 42ms/step - loss: 1.9905 - accuracy: 0.2441 - val_loss: 1.9350 - val_accuracy: 0.3026\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 1.9781 - accuracy: 0.2511 - val_loss: 1.9717 - val_accuracy: 0.3037\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 1.9737 - accuracy: 0.2526 - val_loss: 1.9347 - val_accuracy: 0.3209\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 1.9622 - accuracy: 0.2589 - val_loss: 1.9482 - val_accuracy: 0.3157\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 1.9635 - accuracy: 0.2637 - val_loss: 1.9543 - val_accuracy: 0.3144\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 1.9551 - accuracy: 0.2670 - val_loss: 1.9493 - val_accuracy: 0.3139\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 28s 91ms/step - loss: 1.9474 - accuracy: 0.2699 - val_loss: 1.9312 - val_accuracy: 0.3230\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 1.9196 - accuracy: 0.3316\n",
      "Test accuracy (ANN): 0.33160001039505005\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network model\n",
    "model_ann = models.Sequential()\n",
    "model_ann.add(layers.Dense(512, activation='relu', input_shape=(32 * 32 * 3,)))\n",
    "model_ann.add(layers.Dropout(0.5))\n",
    "model_ann.add(layers.Dense(256, activation='relu'))\n",
    "model_ann.add(layers.Dropout(0.5))\n",
    "model_ann.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_ann.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_ann.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_ann, test_acc_ann = model_ann.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy (ANN): {test_acc_ann}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 113s 169ms/step - loss: 1.7028 - accuracy: 0.3811 - val_loss: 1.5117 - val_accuracy: 0.4613\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 113s 181ms/step - loss: 1.3969 - accuracy: 0.5001 - val_loss: 1.3285 - val_accuracy: 0.5327\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 118s 189ms/step - loss: 1.2787 - accuracy: 0.5459 - val_loss: 1.2679 - val_accuracy: 0.5515\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 111s 177ms/step - loss: 1.1852 - accuracy: 0.5801 - val_loss: 1.2096 - val_accuracy: 0.5725\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 6453s 10s/step - loss: 1.1033 - accuracy: 0.6090 - val_loss: 1.2082 - val_accuracy: 0.5807\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.1933 - accuracy: 0.5765\n",
      "Test accuracy (CNN): 0.5764999985694885\n"
     ]
    }
   ],
   "source": [
    "# Reshape images to (32, 32, 3)\n",
    "train_images = train_images.reshape((train_images.shape[0], 32, 32, 3))\n",
    "test_images = test_images.reshape((test_images.shape[0], 32, 32, 3))\n",
    "\n",
    "# Build the convolutional neural network model\n",
    "model_cnn = models.Sequential()\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.Flatten())\n",
    "model_cnn.add(layers.Dense(64, activation='relu'))\n",
    "model_cnn.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_cnn.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_cnn, test_acc_cnn = model_cnn.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy (CNN): {test_acc_cnn}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((train_images.shape[0], 32, 32, 3))\n",
    "test_images = test_images.reshape((test_images.shape[0], 32, 32, 3))\n",
    "# ResNet18\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Add, Activation, Flatten, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def residual_block(x, filters, stride):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters=filters, kernel_size=(3, 3), strides=stride, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    # Skip connection\n",
    "    if stride > 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=stride, padding='same')(shortcut)\n",
    "    x = Add()([shortcut, x])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet18(input_shape, num_classes):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same', activation='relu')(input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    # Residual blocks\n",
    "    x = residual_block(x, filters=64, stride=1)\n",
    "    x = residual_block(x, filters=64, stride=1)\n",
    "    x = residual_block(x, filters=128, stride=2)\n",
    "    x = residual_block(x, filters=128, stride=1)\n",
    "    x = residual_block(x, filters=256, stride=2)\n",
    "    x = residual_block(x, filters=256, stride=1)\n",
    "    x = residual_block(x, filters=512, stride=2)\n",
    "    x = residual_block(x, filters=512, stride=1)\n",
    "    # Output layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "input_shape = (32,32,3)    \n",
    "model = resnet18(input_shape, 10)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "# Evaluate the model on the test set\n",
    "test_loss_cnn, test_acc_cnn = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy (CNN): {test_acc_cnn}')\n",
    "# model.summary()\n",
    "#tf.keras.utils.plot_model(model,show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
